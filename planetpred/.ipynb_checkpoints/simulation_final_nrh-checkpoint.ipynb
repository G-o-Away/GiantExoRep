{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made modication to code since it kept crashing on get_fscore:<br>\n",
    "alg.booster().get_fscore() --> alg.get_booster().get_fscore()<br>\n",
    "<br>import sys<br>\n",
    "sys.argv = [\"simulation_final_nrh.py\",\"set_test\", \"False\", \"False\", \"True\", \"hypatia-nonCons-noThickDisk-planets-28Feb-nasa.csv\"]<br>\n",
    "execfile(\"simulation_final_nrh.py\")\n",
    "file, set number, golden set (T/F), plot X/Fe (T/F), save hyp plots (T/F)<br>\n",
    "<br>\n",
    "To run:<br>\n",
    "import sys<br>\n",
    "sys.argv = [\"simulation_final_nrh.py\",\"set_test\", \"False\", \"False\", \"True\", \"hypatia-nonCons-noThickDisk-planets-28Feb-nasa.csv\"]<br>\n",
    "execfile(\"simulation_final_nrh.py\")<br>\n",
    "<br>\n",
    "Tutorial for XGBoost:<br>\n",
    " https://jessesw.com/XG-Boost/<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = [\"simulation_final_nrh.py\",\"set_test\", \"False\", \"False\", \"True\", \"hypatia-nonCons-noThickDisk-planets-28Feb-nasa.csv\"]\n",
    "execfile(\"simulation_final_nrh.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml #\n",
    "import pandas as pd #\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from numpy import nan\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import xgboost as xgb #\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\OneDrive\\GradClasses\\Astro278\\PaperRep\\planetPrediction\\planetpred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\O'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\O'\n",
      "C:\\Users\\Yilin\\AppData\\Local\\Temp\\ipykernel_1080\\1520283609.py:2: SyntaxWarning: invalid escape sequence '\\O'\n",
      "  working_dir = \"D:\\OneDrive\\GradClasses\\Astro278\\PaperRep\\planetPrediction\\planetpred\"\n"
     ]
    }
   ],
   "source": [
    "#working_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "working_dir = \"D:\\OneDrive\\GradClasses\\Astro278\\PaperRep\\planetPrediction\\planetpred\"\n",
    "\n",
    "print(os.path.dirname(os.path.realpath(\"__file__\")))\n",
    "#---------------- Definition---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "\n",
    "def set_parameters(set_name, golden_set, input_file):\n",
    "    \n",
    "    \n",
    "    golden = str_to_bool(golden_set)\n",
    "    \n",
    "    #-------------------------------------------------------------------------\n",
    "    \n",
    "    #read in the directory that is being run\n",
    "    data_dir = set_name\n",
    "    \n",
    "    #read in the parameters file and load it\n",
    "\n",
    "    full_path = os.path.join(working_dir,\"{0}\".format(data_dir),'params.yaml')\n",
    "    stream = open(full_path, 'r')\n",
    "    parameters = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "    \n",
    "    #read in Hypatia data as pandas dataframe (2D structure), drop HIP numbers\n",
    "    df  = pd.read_csv(input_file)\n",
    "    \n",
    "    set_number = set_name\n",
    "    \n",
    "    #-------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    if golden:\n",
    "        df2 = df.copy()\n",
    "        df2.loc[df2[(df2['Exo']==1) & (df2['MaxPMass']>parameters['gas_giant_mass'])].sample(10, random_state=np.random.RandomState()).index,'Exo'] = 0\n",
    "        yy = df2.loc[df2['Exo'] == 0].index\n",
    "        zz = df.loc[df['Exo'] == 0].index\n",
    "        changed = [ind for ind in yy if not ind in zz]\n",
    "        changedhips = [df['HIP'][ind] for ind in changed]\n",
    "        df = df2.copy()\n",
    "        yy2 = df2.loc[df2['Exo'] == 0].index\n",
    "        zz2 = df.loc[df['Exo'] == 0].index\n",
    "        changed2 = [ind for ind in yy2 if not ind in zz2]\n",
    "    #-------------------------------------------------------------------------\n",
    "    \n",
    "    df.index        = df['HIP']\n",
    "    df['Exo']       = df['Exo'].astype('category') #category = limited possibilities\n",
    "    df['Multi']     = df['Multi'].astype('category')\n",
    "    df['MaxPMass']  = df['MaxPMass'].astype(np.number)\n",
    "    df['Sampled']   = np.zeros((df.shape[0]))\n",
    "    df['Predicted'] = np.zeros((df.shape[0]))\n",
    "    df = df.drop(['HIP'], 2)\n",
    "    \n",
    "    # Print a bunch of stuff in terminal\n",
    "    print('Parameters used in simulation:')\n",
    "    print('------------------------------')\n",
    "    print('')\n",
    "    \n",
    "    for key in parameters.keys():\n",
    "        print('{0} = {1}'.format(key, parameters[key] ))\n",
    "    \n",
    "    cv_folds = parameters['cv_folds']\n",
    "    early_stopping_rounds = parameters['early_stopping_rounds']\n",
    "    N_iterations = parameters['N_iterations']\n",
    "    N_samples = parameters['N_samples']\n",
    "    gas_giant_mass = parameters['gas_giant_mass']\n",
    "    features = parameters['features']\n",
    "    \n",
    "    relevant_columns = features + ['Exo', 'MaxPMass', 'Sampled', 'Predicted']\n",
    "    \n",
    "    #Redefine dataframe with the \"relevant columns\" and remove nans if dropnans==True in yaml\n",
    "    if(parameters['dropnans']):\n",
    "        df = df[relevant_columns].dropna()\n",
    "    \n",
    "    print('Number of samples used in simulation: {0}'.format(df.shape[0]))\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    #Define the confusion matrix and other arrays\n",
    "    cfm = np.zeros((2,2))\n",
    "    \n",
    "    auc_score_train       = []\n",
    "    precision_score_train = []\n",
    "    feat_imp_train        = pd.DataFrame(columns=features)\n",
    "    probabilities_total   = pd.DataFrame(index = df.index)\n",
    "    \n",
    "    print('iteration \\t estimators')\n",
    "    print('---------------------------')\n",
    "    \n",
    "    #---------------------------XGBOOST LOOP----------------------------------------------\n",
    "    \n",
    "    # Loop for all of the iterations (defined in yaml)\n",
    "    for iteration in range(0, N_iterations):\n",
    "        \n",
    "        #dataframe of 200 random hosts with giant planets\n",
    "        df_iter_with_exo = df[(df['Exo']==1) & (df['MaxPMass']>gas_giant_mass)].sample(N_samples, random_state=np.random.RandomState())\n",
    "        #dataframe of 200 random non hosts\n",
    "        df_iter_none_exo = df[df['Exo']==0].sample(N_samples, random_state=np.random.RandomState())\n",
    "        \n",
    "        # make a new dataframe of the 400 star subset\n",
    "        df_train         = pd.concat([df_iter_with_exo, df_iter_none_exo], axis=0)\n",
    "        # make a dataframe of those stars NOT in the training set (to predict on)\n",
    "        df_predict       = df[~df.index.isin(df_train.index)]\n",
    "        \n",
    "        # The train dataframe with everything but the Exo column\n",
    "        X = df_train.drop(['Exo'],1)\n",
    "        # The Exo column (and hips)\n",
    "        Y = df_train.Exo\n",
    "        \n",
    "        # Note: Using gbtree booster\n",
    "        alg = XGBClassifier(learning_rate =0.1, #def=0.3, prevents overfitting and makes feature weight conservative\n",
    "                            n_estimators=1000, #number of boosted trees to fit\n",
    "                            max_depth=6, #def=6, max depth of tree/complexity\n",
    "                            min_child_weight=1, #def=1, min weight needed to continue leaf partitioning\n",
    "                            gamma=0, #def=0, minimum loss reduction required to make partition on a leaf\n",
    "                            subsample=0.8, #def=1, subsample ratio of the training set\n",
    "                            colsample_bytree=0.8, #def=1, subsample ratio of columns when making each tree\n",
    "                            objective= 'binary:logistic', #def=linear, logistic regression for binary classification, output probability\n",
    "                            nthread=1, #originall = 8, but issue on laptop...def=max, number of parallel threads used to run xgboost\n",
    "                            scale_pos_weight=1, #def=1, balance positive and neg weights\n",
    "                            seed=27) #def=0, random number seed\n",
    "                            \n",
    "        #get input parameters of algorithm\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        \n",
    "        #construct training set matrix\n",
    "        xgtrain = xgb.DMatrix(X[features].values, label=Y)\n",
    "        \n",
    "        #cross validation (CV) of xgboost to avoid overfitting\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds, metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        \n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        print(iteration, '\\t \\t', cvresult.shape[0])\n",
    "        \n",
    "        alg.fit(X[features], Y, eval_metric='auc')\n",
    "    \n",
    "        dtrain_predictions = alg.predict(X[features])\n",
    "        dtrain_predprob    = alg.predict_proba(X[features])[:,1]\n",
    "        \n",
    "        feat_imp        = alg.get_booster().get_fscore()\n",
    "        # See how the algorithm performs on the Exo data\n",
    "        auc_score       = metrics.roc_auc_score(    Y, dtrain_predprob)\n",
    "        precision_score = metrics.precision_score(  Y, dtrain_predictions)\n",
    "        metric_score    = metrics.confusion_matrix( Y, dtrain_predictions)\n",
    "        \n",
    "        # Weighting function to ignore the null values\n",
    "        normalized_features = pd.DataFrame((1 - df_train[features].isnull().sum()/df_train[features].count())* pd.Series(alg.get_booster().get_fscore()), columns=[iteration]).T\n",
    "        \n",
    "        #calculate the confusion matrix\n",
    "        feat_imp_train = pd.concat([feat_imp_train, pd.DataFrame(feat_imp, columns=features, index=[iteration])])\n",
    "        feat_imp_train_normal = pd.concat([feat_imp_train, normalized_features])\n",
    "        auc_score_train.append(auc_score)\n",
    "        precision_score_train.append(precision_score)\n",
    "        cfm += metric_score\n",
    "        \n",
    "        df.loc[df_predict.index, 'Sampled']   += np.ones(len(df_predict.index))\n",
    "        df.loc[df_predict.index, 'Predicted'] += alg.predict(df_predict[features])\n",
    "        df.loc[df_predict.index, 'Prob']       = alg.predict(df_predict[features])\n",
    "        \n",
    "        values = df['Prob']\n",
    "        probabilities_total = pd.concat([probabilities_total, pd.Series(values, name=str(iteration))], axis=1)\n",
    "    \n",
    "        if(not iteration % 10):\n",
    "            probabilities_total.to_pickle('{0}/probabilities_total.pkl'.format(data_dir))\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------------------\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    cfm /= N_iterations\n",
    "    cfm[0] /= cfm[0].sum()\n",
    "    cfm[1] /= cfm[1].sum()\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(np.round(cfm, 3))\n",
    "    df['Prob'] = df['Predicted'] / df['Sampled']\n",
    "    \n",
    "    ###########-------------------Output List of Planets------------------------#########\n",
    "    \n",
    "    #Find the stars with >90% probability of hosting a planet, with the Sampled, Predicted, and Prob columns\n",
    "    planets = df[(df.Prob>.90) & (df.Exo==0)][['Sampled', 'Predicted', 'Prob']]\n",
    "    print('Number of most probable planet hosts: {0}'.format(planets.shape[0]))\n",
    "    \n",
    "    #Sort the stars with predicted planets and save that file\n",
    "    planetprobs = planets.sort_values(by='Prob', ascending=False)\n",
    "    name = data_dir+'/figures/planet_probabilities'+str(datetime.today().strftime('-%h%d-%H%M'))+'.csv'\n",
    "    #name = data_dir+'/figures/planet_probabilities.csv'\n",
    "    outfile = open(name, 'w')\n",
    "    planetprobs.to_csv(outfile)\n",
    "    outfile.close()\n",
    "    \n",
    "    #Create a second list with all stars in Hypatia and the probabilities\n",
    "    planets2 = df[(df.Prob>.0) & (df.Exo==0)][['Sampled', 'Predicted', 'Prob']]\n",
    "    if golden: #if 10 stars were randomly taken out\n",
    "        changeddf = pd.DataFrame([]) #make empty dataframe\n",
    "        for star in changedhips:  #loop over the 10 known planets hosts (defined at top)\n",
    "            changeddf = changeddf.append(planets2.loc[planets2.index==star])\n",
    "            if planets2.loc[planets2.index==star].empty: #catch for when a known planet host was cut (bc of abunds)\n",
    "                temp = pd.Series([nan,nan,nan], index=['Sampled', 'Predicted', 'Prob'])\n",
    "                temp.name = star\n",
    "                changeddf = changeddf.append(temp) #append blank file (with star name as index)\n",
    "        #Save golden set as a separate file with the date and time as a tag\n",
    "        filename ='{0}/figures/goldenSetProbabilities'+str(datetime.today().strftime('-%h%d-%H%M'))+'.csv'\n",
    "        changeddf.to_csv(filename.format(set_number), na_rep=\" \")\n",
    "    \n",
    "    #Save the file with all of the probabilities\n",
    "    planetprobs2 = planets2.sort_values(by='Prob', ascending=False)\n",
    "    name2 = data_dir+'/figures/planet_probabilitiesAll'+str(datetime.today().strftime('-%h%d-%H%M'))+'.csv'\n",
    "    #name2 = data_dir+'/figures/planet_probabilitiesAll.csv'\n",
    "    outfile2 = open(name2, 'w')\n",
    "    planetprobs2.to_csv(outfile2)\n",
    "    outfile2.close()\n",
    "    \n",
    "    ###########------------------------Save Files------------------------##########\n",
    "    print('Saving data files')\n",
    "    \n",
    "    #Save files\n",
    "    feat_imp_train.to_pickle('{0}/features_train.pkl'.format(data_dir))\n",
    "    feat_imp_train_normal.to_pickle('{0}/features_train_normal.pkl'.format(data_dir))\n",
    "    probabilities_total.to_pickle('{0}/probabilities_total.pkl'.format(data_dir))\n",
    "    df.to_pickle('{0}/df_info_all.pkl'.format(data_dir))\n",
    "    \n",
    "    np.save('{0}/auc_score_train.npy'.format(data_dir), np.array(auc_score_train))\n",
    "    np.save('{0}/precision_score_train.npy'.format(data_dir), np.array(precision_score_train))\n",
    "    np.save('{0}/cfm.npy'.format(data_dir), cfm)\n",
    "    \n",
    "    print('Simulation completed successfully.')\n",
    "    if golden:\n",
    "        print(\"Changed indices and HIP numbers:\")\n",
    "        print(changed)\n",
    "        print(changedhips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
